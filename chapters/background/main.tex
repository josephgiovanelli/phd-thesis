\chapter{Background}
\label{chap:background}

Alan Turing defines the field of AI in cognitive terms \cite{turing1980computing}, raising the question of whether machines can show intelligence, and along this line Arthur Samuel defines ML \cite{samuel2000some}, building upon the notion of intelligence as learning.
A more formal definition is provided by Tom M. Mitchell \citep{mitchell1997machine}, who delineates the behavior of algorithms studied in machine learning and introduces the concepts of \textit{task} and \textit{experience}.
\begin{definition}[Machine Learning \citep{mitchell1997machine}]
    A computer program is said to learn in some class of tasks, with respect to a performance measure, if its performance improves with experience.
\end{definition}
Notably, the formalization of experience is contingent upon the task to address, which is crucial because determines what the ML algorithm can observe and, in turn, how it operates.
Specifically, the literature identifies three different ML tasks: \textit{supervised}, \textit{unsupervised}, and \textit{reinforcement learning}.

\paragraph{Supervised learning} This is the kind of learning studied in most current research in the ML field. The algorithm is provided with a sample of data describing scenarios that have been \textit{experienced} in the past along with the corresponding \textit{ground truth}---i.e., values that indicate the correct outcomes.
The goal consists of finding a \textit{function} that maps samples to outcomes so that the next unseen scenarios can be labeled correctly.


\paragraph{Unupervised learning}
Here, the samples come with no ground truth available.
The algorithm has to guess the outcome by uncovering hidden insights or patterns, investigating different partitionings, or estimating density distributions within the data.
Unsupervised tasks are inherently exploratory in nature and are often employed in data mining to provide explanations for the given scenarios or even to summarize them.

\paragraph{Reinforcement Learning}
Standing on the edge of the ML landscape, it has been categorized as the learning that differentiates the most from ``classical'' tasks \cite{sutton2018reinforcement}.
Here the experience is not sampled and fed to the algorithm but rather comes from interactions with an environment.
The algorithm learns by making decisions and receiving feedback based on its actions, allowing it to adapt its behavior over time.\\

%TODO: Qua c'è da menzionare che infatti automatizzare RL, in letteratura, prende il nome di AutoRL mentre noi facciamo AutoML.
This thesis focuses on supervised and unsupervised learning.
% providing a formalization that covers both cases.
Particularly in this chapter, \Cref{automl-background-sec:ml} introduces the building blocks such as dataset, algorithm, and hyperparameter; while
% provides the necessary background in ML,
\Cref{automl-background-sec:automl} formalizes the problems tackled by automated machine learning (AutoML), up to the combined algorithm selection and hyperparameter optimization.
% along with the state-of-the-art techniques to solve them.
% and finally \Cref{automl-background-sec:formalization} extends the formalization towards a more operational level of ML, enabling the user to handle comprehensive ML pipelines---i.e., concatenations of steps that cover more than the sole learning process.
% Here we can also put something like " ... extends the formalization focusing on a more operational level of ML and methods that following this way enables to handle comprehensive ... "

\section{Machine Learning}\label{automl-background-sec:ml}

At the core of each and every task lies the data, serving as the essential fuel for the learning process because providing the means to observe the real world.
Specifically in ML, we refer to a dataset $\altmathcal{D}$ as a sample of data from which it is possible to learn that experience of interest.

\begin{definition}[Dataset]\label{def:dataset}
    A \textit{dataset} $\altmathcal{D}$ is a collection of data points $X$, with corresponding space $\altmathcal{X}$.
    According to the task $T$, the dataset may include the ground truth $Y$ within its corresponding space $\altmathcal{Y}$ (supervised) or not (unsupervised).
    \begin{equation*}\label{eq:dataset}
        \altmathcal{D} = \left\lbrace\,
        (d_i)_{i=1}^N  \quad \begin{array}{|@{}l@{}l@{\quad}l@{}}
            \quad d_i = (\pmb{x}_i, y_i) & \in \mathbb{D} \subset \altmathcal{X} \times \altmathcal{Y} & \text{{if }} T = \text{{supervised}} \\
            \quad d_i = \pmb{x}_i & \in \mathbb{D} \subset \altmathcal{X} & \text{{if }} T = \text{{unsupervised}}
        \end{array}
        \right\rbrace
    \end{equation*}
\end{definition}

Intuitively, a dataset resembles a structured table where each row represents a unique instance $x \in X$ characterized by specific features in their domain $\altmathcal{X}$.
The features, denoted by the columns, describe the diverse factors or characteristics influencing a particular outcome $Y$ in its domain $\altmathcal{Y}$.
While in supervised learning the ground truth is provided with the aim of understanding the hidden relationships between features and corresponding outcomes, in supervised tasks, the most appropriate outcome has to be found by discovering patterns within the data.

\begin{example}[Iris dataset \cite{iris}]\label{ex:dataset}
    The iris dataset contains $150$ instances of flowers ($X$) under four features in cm ($\altmathcal{X} \subset \mathbb{R}^4$): sepal length, sepal width, petal length, and petal width.
    The data have been collected to quantify the morphologic variation within the Iris species ($Y$), which can assume the following $3$ classes ($\altmathcal{Y}$): Iris setosa, Iris virginica and Iris versicolor.
    \begin{table}[!h]
        \centering
        \begin{tabular}{llll|l}
            \hline
            Sepal length & Sepal width & Petal length & Petal width & Class \\ \hline
            % $5.1$ & $3.5$ & $1.4$ & $0.2$ & Iris-setosa \\
            $4.9$ & $3.0$ & $1.4$ & $0.2$ & Iris-setosa \\
            % $7.0$ & $3.2$ & $4.7$ & $1.4$ & Iris-versicolor \\
            $6.4$ & $3.2$ & $4.5$ & $1.5$ & Iris-versicolor \\
            % $6.3$ & $3.3$ & $6.0$ & $2.5$ & Iris-virginica \\
            $5.8$ & $2.7$ & $5.1$ & $1.9$ & Iris-virginica \\ \hline
            \
        \end{tabular}
        \caption{Example tuples from the Iris dataset.}
        \label{tbl:iris}
    \end{table}

    \noindent This dataset is also used in unsupervised learning tasks by discarding the class attribute, yet the data only contains two clusters with rather obvious separation.
\end{example}

% The aim is to discover the hidden relationships between inputs $X$ and outputs $Y$, hence learning a function $f: \altmathcal{X} \rightarrow \altmathcal{Y}$.
% Despite the task,
The ultimate goal is learning a function $f: \altmathcal{X} \rightarrow \altmathcal{Y}$.
A (machine) learning \textit{algorithm} $A$ leverages data points in $\altmathcal{D}$ to estimate such a function $f$, which is expressed via a vector of \textit{model parameters} $H$.
Most algorithms further expose hyperparameters $\lambda_1, \dots, \lambda_K$ that change the functioning of the algorithm itself.

\begin{definition}[Machine Learning Algorithm]\label{def:algorithm}
    Given the hyperparameters $\lambda_1, \dots, \lambda_M$, with corresponding spaces $\Lambda_1, \dots, \Lambda_M$, we refer with $\pmb{\lambda}$ to a hyperpaprameter configuration sampled from the space $\pmb{\Lambda} = \Lambda_1 \times \dots \times \Lambda_M$.
    Then, given a dataset $\altmathcal{D} \in \mathbb{D}$ and a hyperparameter configuration $\pmb{\lambda} \in \pmb{\Lambda}$, an algorithm $A(\altmathcal{D}, \pmb{\lambda})$  provides a model from the model space $\mathbb{H}$.
    \begin{equation*}\label{eq:algorithm}
        A: \mathbb{D} \times \pmb{\Lambda} \rightarrow \mathbb{H}
    \end{equation*}
    For brevity, we refer to $A(\altmathcal{D}, \pmb{\lambda})$ with the notation $A_{\pmb{\lambda}}(\altmathcal{D})$, and $H_{\pmb{\lambda}}$ for the resulting trained model.
    % For conciseness matter, we write $A_{\pmb{\lambda}}$ for $A(\altmathcal{D}, \pmb{\lambda})$ and refer to $H_{\pmb{\lambda}}$ as the corresponding model.
\end{definition}

The actual learning is performed via the so-called \textit{model training}, also known as \textit{model fitting} due to the iterative process of adjusting the internal parameters of the model until convergence.
The quality of such a model heavily depends on the hyperparameter choices we made, hence we need loss functions that assess the quality of different configurations.
% How well a model performs depends heavily on the hyperparameter choices we make and loss functions assess the quality of such configurations.
The term loss typically refers to the error committed by the model, hence the lower the better; instead, when higher values are favored, we refer to quality metrics.
Quality metrics share the same signature as the losses.

\begin{definition}[Loss Function]\label{def:loss}
    Given a dataset $\altmathcal{D} \in \mathbb{D}$ and a model $H_{\pmb{\lambda}} \in \mathbb{H}$, the loss function $\altmathcal{L}(\altmathcal{D}, H_{\pmb{\lambda}})$ quantifies how well the given model performs on $\altmathcal{D}$.
    \begin{equation*}\label{eq:loss}
        \altmathcal{L}: \mathbb{H} \times \mathbb{D} \rightarrow \mathbb{R}
    \end{equation*}
\end{definition}

In the following, we walk over the main characteristics and differences between the specific cases of supervised and unsupervised learning.

\subsection{Supervised Learning}

Supervised learning tasks can be differentiated based on the nature of the desired outcome.
\begin{itemize}
    \item \textbf{Classification} tasks, the outcome assumes finite values, representing the class to which the instances belong e.g., in an image recognition use-case labels may include ``dog'', ``cat'', ``car'', or ``tree''.
    \item \textbf{Regression} tasks, the outcome is continuous and -- according to the semantic of the task -- can be considered as either forecasting or estimation e.g., predicting future stock prices based on historical data or estimating soil moisture based on soil features and weather conditions.
\end{itemize}
Algorithms employ model training to minimize the discrepancy between the predictions and the provided ground truth.
However, the aim is to acquire the knowledge to perform well on new, future data, hence achieving a good \textit{generalization}.
Two undesired scenarios can occur.
\textit{Overfitting} takes place when the model becomes too complex and fits the training data too closely, not being able to generalize the acquired knowledge to unseen data, capturing noise rather than genuine patterns.
On the contrary, \textit{underfitting} happens when the model remains too simplistic, failing to capture the underlying complexities.
Hyperparameters are crucial in striking the right balance.

\begin{example}[Decision Tree]
    \label{ex:decision_tree}
    The decision tree algorithm recursively splits instances based on their feature values, creating a hierarchical tree-like structure where each leaf represents a class or a prediction value---in classification and regression tasks respectively.
    % Notably, in its supervised nature, the tree can be built by minimizing the entropy of the leaves.
    We can control the complexity of such a tree by leveraging hyperparameters such as the maximum depth of the tree and the minimum number of instances in the leaves.
    In this case, a deeper tree with more splits may capture intricate patterns but is prone to overfitting, as to a shallower tree, it might generalize better but may overlook finer details.
\end{example}

Loss functions are inevitable when it comes to assessing generalization performance but -- when we are provided with solely one set of data -- their application should follow protocols that assure consistency and statistical reliability.
The most common protocol involves splitting the original dataset $\altmathcal{D}$ into two disjoint sets $\altmathcal{D}_\mathit{train}$ and $\altmathcal{D}_\mathit{valid}$, where the model is trained only based on $\altmathcal{D}_\mathit{train}$ but validated with $\altmathcal{L}$ on $\altmathcal{D}_\mathit{valid}$.
Yet, both model training and validation demand substantial amounts of data.
When the split is not feasible, a practical solution is the $k$-fold cross-validation technique.
This divides the dataset into $k$ folds and, for each fold, uses the corresponding subset of data for testing while employing the remaining $(k - 1)$ folds for training.

\begin{definition}[$K$-Fold Cross-Validation]
    The $k$-fold cross-validation provides a protocol to validate an algorithm $A_{\pmb{\lambda}}$ via a loss $\altmathcal{L}$ on a dataset $\altmathcal{D}$.
    \begin{equation*}
        \frac{1}{k}~\mathlarger{\sum}_{i=1}^{k} \altmathcal{L}(A_{\pmb{\lambda}}(\altmathcal{D}_{train}^{(i)}), \altmathcal{D}_{valid}^{(i)})
    \end{equation*}
    where $\altmathcal{D}_{train}^{(i)}$ stands for $\altmathcal{D}$ without the $i^{\textup{th}}$ fold (used for validation) i.e., $\altmathcal{D}_{train}^{(i)} = \altmathcal{D} \setminus \altmathcal{D}_{valid}^{(i)}$.
\end{definition}

Follows two examples of losses: misclassification error (\Cref{ex:misclassification}) and root mean square error (\Cref{ex:rmse}), for classification and regression tasks respectively.
In both cases, the evaluation is done by comparing predictions with the ground through.

\begin{example}[Misclassification Error]\label{ex:misclassification}
    Given the validation set $(\pmb{x_i}, y_i) \in D_\mathit{valid}$  and the model $H_{\pmb{\lambda}}$ resulted from the training $A_{\pmb{\lambda}}(D_\mathit{train})$, then the loss MIS computes the fraction of incorrect predictions made by $H$.
    \begin{equation*}
        MIS(H_{\pmb{\lambda}}, \altmathcal{D}_\mathit{valid}) = \frac{1}{|D_\mathit{valid}|}~\mathlarger{\sum}_{(\pmb{x_i}, y_i) \in D_\mathit{valid}} \left\lbrace \begin{array}{ll}
            1 & if (H_{\pmb{\lambda}}(\pmb{x_i}) \ne y_i) \\
            0 & otherwise
        \end{array}
        \right.
    \end{equation*}
    In classification tasks, the accuracy ($ACC = 1 - MIS$) computes the fraction of incorrect predictions and is often used as a quality metric.
\end{example}


\begin{example}[Root Mean Square Error]\label{ex:rmse}
    Given the validation set $(\pmb{x_i}, y_i) \in D_\mathit{valid}$  and the model $H_{\pmb{\lambda}}$ resulted from the training $A_{\pmb{\lambda}}(D_\mathit{train})$, then the loss RMSE computes the average difference between the predictions made by $H$ and the actual values in $D_\mathit{valid}$.
    \begin{equation*}
        RMSE(H_{\pmb{\lambda}}, \altmathcal{D}_\mathit{valid}) = \sqrt{\frac{1}{|D_\mathit{valid}|}~\mathlarger{\sum}_{(\pmb{x_i}, y_i) \in D_\mathit{valid}} (H_{\pmb{\lambda}}(\pmb{x_i}) - y_i)^2}
    \end{equation*}
\end{example}

\subsection{Unsupervised Learning}

According to the desired outcome, unsupervised learning provides a categorization similar to supervised one.
\begin{itemize}
    \item \textbf{Cluster analysis}, the outcome can assume finite values and it aims to group instances in such a way that those in the same group (called a cluster) are more similar to each other than to those in other groups e.g., identifying customer segments based on purchasing behavior.
    \item Otherwise, when the outcome is continuous, there is no nomenclature in literature and tasks are named after the semantic at hand e.g., anomaly detection assigns an outlier score to each instance that deviates from the overall distribution.
\end{itemize}
% The core of unsupervised learning lies in training not to explicitly perform the task on future instances but, rather, to extract patterns and insights from the training data itself.
However, in unsupervised scenarios, the knowledge acquired through model fitting does not have the goal of predicting future instances but, rather, of understanding the inherent patterns and structure within the training data itself.
Popular examples of algorithms are: k-means \cite{k_means}, isolation forest \cite{random_forest}

\begin{example}[K-Means \cite{k_means}]
    The k-means algorithm aims to partition the instances into a predetermined number of clusters based on their feature similarity.
    The algorithm iteratively assigns instances to clusters and updates the cluster centroids until its convergence criteria i.e., instances in the same cluster are more similar than instances in a different cluster.
    Hyperparameters such as the number of clusters and the similarity measure affect the returned partitioning.
\end{example}

Consistently, the model evaluation is performed atop the training dataset and does not need any particular protocol.
Being provided without any ground truth, loss functions and quality metrics estimate the model performance based on patterns and relationships discovered within the feature values themselves.
For instance, in cluster analysis, the aim is to find a well-separated partitioning (also known as clustering).
The Silhouette index (\Cref{ex:sil}) is a quality metric that estimates: (i) cohesion of the provided clusters as the mean distance between a sample and all other points in the same cluster, and (i) separability as the mean distance between a sample and all other points in the next nearest cluster.

\begin{example}[Silhouette Index \cite{sil}]\label{ex:sil}
    Given the dataset set $\altmathcal{D}$ and the model $H_{\pmb{\lambda}}$ resulted from the training $A_{\pmb{\lambda}}(\altmathcal{D})$, then the quality metric $SIL$ summarizes cohesion $COH$ and separability $SEP$ of the clustering made by $H_{\pmb{\lambda}}$
    \begin{equation*}
        COH(H, \pmb{x_i}) = \frac{1}{|C_{i}| - 1} \sum_{\pmb{x} \in C_{i}, \pmb{x} \neq x_i} d(\pmb{x}, \pmb{x_i}) \quad
        SEP(H, \pmb{x_i}) = \min_{i \neq j} \frac{1}{|C_j|} \sum_{\pmb{x_j} \in C_j} d(\pmb{x_j}, \pmb{x_i})
    \end{equation*}
    \begin{equation*}
        SIL(H, \altmathcal{D}) = \frac{1}{|\altmathcal{D}|}~\mathlarger{\sum}_{\pmb{x_i} \in \altmathcal{D}} \frac{COH(H_{\pmb{\lambda}}, x_i) - SEP(H, \pmb{\pmb{x_i}})}{\max(COH(H, \pmb{x_i}), SEP(H, \pmb{x_i}))}
    \end{equation*}
    where $C_i$ is the cluster of instances to which $x_i$ belongs i.e., $C_i = \{\pmb{x} \in \altmathcal{D} | H_{\pmb{\lambda}}(\pmb{x}) = H_{\pmb{\lambda}}(\pmb{x_i})\}$ and $d(\pmb{x}_i, \pmb{x}_j)$ is the distance between the data points $\pmb{x}_i$ and $\pmb{x}_j$.
\end{example}




\section{Automated Machine Learning}\label{automl-background-sec:automl}


The process of finding the best-performing machine learning model for a given dataset involves two kinds of choices: selecting a learning algorithm and tuning it by setting its hyperparameters.
In the following, we define the corresponding optimization problems.

Let us consider a dataset $\altmathcal{D} = \{d_1, \dots, d_n\} \in \mathbb{D}$.
If the task $T$ is supervised, we refer with $\altmathcal{D}_{train}$ and $\altmathcal{D}_{valid}$ to two different data splits for training and validation respectively; otherwise, if $T$ is unsupervised, they correspond to the same original dataset $\altmathcal{D}$.
Besides, we are provided with a loss function $\altmathcal{L} = \mathbb{H} \times \mathbb{D} \rightarrow \mathbb{R}$.

\begin{definition}[Algorithm Selection (AS)]
    Given a set of learning algorithms $\altmathcal{A} = \{A_1, \dots, A_K\}$, the goal of algorithm selection is to determine the algorithm $A^{\star} \in \altmathcal{A}$ with optimal loss function
    \begin{equation*}
        A^{\star} \in \argmin_{A_i \in \altmathcal{A}} \altmathcal{L}(A_i(\altmathcal{D}_{train}), \altmathcal{D}_{valid})
    \end{equation*}
    where $A_i$ is trained with its default hyperparameter setting.
\end{definition}

\begin{definition}[Hyperparameter Optimization (HPO)]
    Given an algorithm $A$ and hyperparameters $\lambda_1, \dots, \lambda_M$, with the corresponding cross product space of $\pmb{\Lambda} = \Lambda_1 \times \dots \times \Lambda_M$, the goal of hyperparameter optimization is to determine the configuration $\pmb{\lambda}^{\star} \in \pmb{\Lambda}$ with optimal loss function.
    \begin{equation*}
        \pmb{\lambda}^{\star} \in \argmin_{\pmb{\lambda} \in \pmb{\Lambda}} \altmathcal{L}(A_{\pmb{\lambda}}(\altmathcal{D}_{train}), \altmathcal{D}_{valid})
    \end{equation*}
\end{definition}

There has been considerable past work separately addressing algorithm selection \cite{as_algos} and hyperparameter optimization \cite{hpo_algos}.
One could similarly tackle the combined space of learning algorithms and their hyperparameters (CASH), yet this has not achieved good results.
The corresponding loss function becomes noisy when the space is high-dimensional, involves both categorical and continuous choices, and contains hierarchical dependencies (e.g., the hyperparameters of a learning algorithm are only meaningful if that algorithm is chosen).
For such reasons CASH has been reformulated as a single combined hierarchical hyperparameter optimization with space $\pmb{\Lambda} = \pmb{\Lambda_1} \cup \dots \cup \pmb{\Lambda_K} \cup \{\lambda_r\}$, where $\lambda_r$ is a new root-level hyperparameter that selects between algorithms $A_1, \dots, A_K$.
The root-level parameters of each subspace $\pmb{\Lambda_i}$ are made conditional on $\lambda_r$ being instantiated to $A_i$.

\begin{definition}[Combined Algorithm Selection and Optimization (CASH)]
    Given a set of learning algorithms $\altmathcal{A} = \{A_1, \dots, A_K\}$, with assocciated hyperparameter spaces  $\pmb{\Lambda}_1, \dots, \pmb{\Lambda}_K$, we define the hierarchical space $\pmb{\Lambda} = \pmb{\Lambda_1} \cup \dots \cup \pmb{\Lambda_K} \cup \{\lambda_r\}$. Then, the goal is to find the algorithm and corresponding configuration $A^{\star}_{\pmb{\lambda}^{\star}} \in \pmb{\Lambda}$ with optimal loss function.
    \begin{equation*}
        A^{\star}_{\pmb{\lambda}^{\star}} \in \argmin_{A_{\pmb{\lambda}} \in \pmb{\Lambda}} \altmathcal{L}(A_{\pmb{\lambda}}(\altmathcal{D}_{train}), \altmathcal{D}_{valid})
    \end{equation*}
\end{definition}

The loss function serves as the objective i.e., it represents the goal of the optimization process.
Specifically, our objective is considered a black box, namely a costly function in which no assumptions on the distribution can be made \cite{69, 134, 88, 55}.
Several approaches have been based on and borrowed ideas from the domains of statistical and traditional optimization \cite{opt_algos}.
Besides, depending on the number of objectives in which the user is interested, we categorized them as either single- or multi-objective.
In the following, we showcase the challenges of both.

\subsection{Single-objective Optimization}

We provide the most common approaches for tackling single-objective optimization, in particular: the baselines \textit{exhaustive}, \textit{grid}, and \textit{random search};  the \textit{model-free} ones such as \textit{simulated annealing}, and the approaches based on \textit{evolutionary} and \textit{multi-fidelity}; and finally the \textit{model-based} framework of \textit{bayesian optimization} (BO).
Unlike model-free approaches, the model-based framework leverages a surrogate model that drives the optimization toward the objective function and BO is the current leader of black-box optimization techniques.
Having gained significant attention for tuning huge search spaces across various tasks, including classification and regression \cite{113, 112}, we will use it throughout the whole thesis.

\subsubsection{Baselines}
The baseline techniques share the common characteristic of exploring the hyperparameter space without any intelligent strategies, relying instead on systematic or random exploration of the hyperparameter space.

\paragraph{Exhaustive search}
Being the most naive solution, exhaustive search considers all combinations of hyperparameters.
Obviously, this is computationally expensive and suffers from the curse of dimensionality as the number of trials grows exponentially with the number of hyperparameters \cite{7}, resulting in unfeasible.

\paragraph{Grid Search}
Another basic solution consists of choosing a predefined set of values for each hyperparameter and computing all combinations in the corresponding cross product.
This method allows for a good exploration, yet it assumes the loss function being uniformly distributed across all hyperparameters.
Besides, even in this case, the number of trials grows exponentially with the number of hyperparameters.

\paragraph{Random Search}
Given a budget $B$, in terms of either time or number of trials, this method samples configurations at random until $B$ is exhausted \cite{10}.
Random search is often considered a baseline since it tends to find better solutions than grid search \cite{90} and it is -- surprisingly -- competitive with most state-of-the-art approaches.
One of the main advantages of random search, and grid search, is that they can be easily parallelized over a number of workers, which is essential when dealing with big data.

\subsubsection{Model-free Optimization}
Model-free approaches share a commonality in their reliance on heuristic techniques and exploration strategies that do not explicitly model the objective function or employ gradient information.

\paragraph{Simulated Annealing}
This optimization technique draws inspiration from metallurgical processes involving controlled heating and cooling \cite{61}.
Initially, a single configuration is randomly chosen and evaluated, this consists of the initial state.
Next, one of the hyperparameters is randomly updated by selecting a value from its immediate neighborhood, to obtain a neighboring state, and the performance is assessed.
The obtained performances from the current and neighboring states are then compared and, finally, the user decides to accept or reject the neighboring state as the current state.

\paragraph{Evolutionary Approaches}
Evolutionary (or genetic) approaches draw inspiration from the process of natural gene selection \cite{49}.
The fundamental concept behind genetic-based optimization techniques is the application of multiple genetic operations to a population of configurations.
For instance, the crossover operation involves taking two parent chromosomes (configurations) and combining their genetic information to generate new offspring.
Specifically, the two parent configurations are cut at the same crossover point, and the sub-parts to the right of that point are swapped between the two parent chromosomes, resulting in two new offspring (child configurations).
Mutation randomly selects a chromosome and mutates one or more of its parameters, leading to a completely new chromosome.

\paragraph{Multi-fidelity approaches}
Here, samples can be evaluated at different levels, specifically: high-fidelity evaluation and low-fidelity evaluation.
High-fidelity evaluation provides precise results from the entire dataset, while low-fidelity evaluation offers a more economical assessment from a subset of the dataset (e.g., folds in a k-fold cross-validation).
The core concept relies on leveraging multiple low-fidelity evaluations to reduce the overall evaluation cost.
Although low-fidelity optimization may lead to a cheaper evaluation cost, it might compromise optimization performance.
Nevertheless, the achieved speedup is often more substantial than the introduced approximation error.

\subsubsection{Model-based Optimization}

In principle, employing a framework in which a surrogate model guides the optimization process translates into building a probability distribution that can approximate the objective function based on previous evaluations, and hence understand the most promising configurations.
As one might expect, we refer to bayesian optimization due to the well-known bayesian theorem, describing the probability of an event based on prior knowledge of conditions that might be related to the event.
% \begin{equation*}
%     P(c(\pmb{\lambda}) | h) = \frac{P(h | c(\pmb{\lambda})) \cdot P(c(\pmb{\lambda}))}{P(h)}
% \end{equation*}
Similarly, we can get the estimated cost of a hyperparameter configuration $c(\pmb{\lambda})$ given the history of previously evaluated configurations $h$.
The algorithm of sequential model-based optimization (SMBO) is described in \Cref{alg:smbo}.
\begin{algorithm}
    \caption{SMBO Algorithm}
    \label{alg:smbo}
    \begin{algorithmic}[1]
        \Require{$B$: budget for optimization}
        \State Initialize surrogate $S$ and history $h \leftarrow \emptyset$
        \While{$B$ has not been exhausted}
            \State $\pmb{\lambda} \leftarrow$ candidate configuration from $S$
            \State Compute $c(\pmb{\lambda}) = \altmathcal{L}(A_{\pmb{\lambda}}(\altmathcal{D}_{train}), \altmathcal{D}_{valid})$
            \State $h \leftarrow h \cup \{(\pmb{\lambda}, c)\}$
            \State Update $S$ given $h$
        \EndWhile
        \State \textbf{return} $\pmb{\lambda}$ from $h$ with minimal $c$
    \end{algorithmic}
\end{algorithm}

\noindent This begins by building a model $S$ that captures the dependence of the loss function $L$ on hyperparameter settings $\pmb{\lambda}$ (line 1).
It then iterates through the following steps: using $S$ to determine a promising candidate configuration of hyperparameters $\pmb{\lambda}$ to evaluate next (line 3); evaluating the loss $c$ of $\pmb{\lambda}$ (line 4); and updating the model $S$ with the new data point $(\pmb{\lambda}, c)$ obtained (lines 5–6).
To select its next hyperparameter configuration $\pmb{\lambda}$ using the model $S$, SMBO employs a so-called acquisition function $a_{S} : \pmb{\Lambda} \rightarrow \mathbb{R}$, which uses the predictive distribution of $S$ at arbitrary hyperparameter configurations $\pmb{\lambda} \in \pmb{\Lambda}$ to quantify (in closed form) how useful knowledge about $\pmb{\lambda}$ would be.
SMBO then maximizes this function over $\pmb{\Lambda}$ to select the most promising configuration $\pmb{\lambda}$ to evaluate next.
Several prominent acquisition functions exist [17, 23, 26], all aiming to automatically trade off exploitation (locally optimizing hyperparameters in regions known to perform well) versus exploration (trying hyperparameters in a relatively unexplored region of the space) to avoid premature convergence.
An example of a promising acquisition function is the probability of improvement $PI = P(c(\pmb{\lambda}) \ge c(\hat{\pmb{\lambda}}))$, where $\hat{\pmb{\lambda}}$ corresponds to the best hyperparameter configuration encountered so far during the optimization.

However, bayesian optimization achievements are due to ever-efficient implementations of surrogate models.
Follows the more important ones.

\paragraph{Gaussian processes (GP)} Commonly considered the pioneers among the surrogates \cite{112, 80}, yet suffering from cubic complexity with the number of data points, limiting parallelization capability.

\paragraph{Tree-structured parzen estimator (TPE)} \cite{13}, in contrast, does not define a predictive distribution over the objective function but creates two density functions for all the hyperparameters in the problem.
Based on past observations, a percentile for each hyperparameter is built to distinguish good and bad observations.
The hyperparameter values are split into two sets, and their respective distributions are determined using density estimation through \textit{parzen windows}\footnote{Statistical technique to estimate a density distribution by generalization from a histogram.}.
The ratio between the two density functions reflects the improvement in the acquisition function and is used to recommend new configurations.
TPE has demonstrated excellent performance in hyperparameter optimization tasks \cite{13, 12, 35, 37, 115}.

\paragraph{Sequential model-based algorithm configuration (SMAC)} Random forest \cite{20} is leveraged as a regressor to model the posterior distribution of the surrogate i.e., the well-known ML model is used to estimate the cost as in a usual regression task.
The algorithm starts by training different regression trees, each built using randomly selected data points.
For each tree, we are provided with an estimation cost, which translates into the posterior distribution in the bayesian framework.

\paragraph{BOHB}
\textcolor{orange}{TODO}


\paragraph{FLAML}
\textcolor{orange}{TODO}

\subsection{Multi-objective Optimization}


When it comes to learning a machine learning model with more than one (possibly conflicting) objective or loss function $\altmathcal{L}_1, \dots, \altmathcal{L}_M$ in mind, comparing the quality of models becomes difficult.
For instance, in the context of Green AI, searching for a model with higher performance usually leads to one with higher power consumption, and vice versa.
In multi-objective (MO) ML, we leverage the concept of dominance i.e., a model $H_1 \in \mathbb{H}$ is better than another $H_2 \in \mathbb{H}$, if $H_1$ performs better in at least one of the objectives while not performing worse than $H_2$ in the remaining ones.
Given a set of models $\altmathcal{H} \subset \mathbb{H}$ and $\altmathcal{L}_1, \dots, \altmathcal{L}_M$ objectives, we are provided with a Pareto Front i.e., a set of non-dominated models that are indistinguishable with respect to this dominance idea.

\begin{definition}[Pareto Front]
    A Pareto front $P(H)$ evaluated on dataset $\altmathcal{D}_{\mathit{val}}$ for a given set of models $\altmathcal{H} \in \mathbb{H}$ is the set of solutions that are non-dominated i.e., as good or better in all objectives and better in at least one objective.

    \begin{equation*}
        \label{eq:pareto_def}
            P_{\altmathcal{D}_{\mathit{val}}}(\altmathcal{H}) = \left\{ H \,\, \begin{array}{|l}
            H \in \altmathcal{H}, \nexists H' \in \altmathcal{H},
            \forall m \in \{1, \dots, M\} : \\
            \quad\quad\altmathcal{L}_m(H',\altmathcal{D}_{\mathit{val}}) \leq \altmathcal{L}_m(H,\altmathcal{D}_{\mathit{val}}), \\
            \exists j \in \{1, \dots, M\} : \\
            \quad\quad\altmathcal{L}_j(H',\altmathcal{D}_{\mathit{val}}) < \altmathcal{L}_j(H, \altmathcal{D}_{\mathit{val}})
            \end{array}\right\} \, .
        \end{equation*}
\end{definition}

In practice, the ultimate goal of a MO task consists of finding the best Pareto Front.
On the one hand, there are approaches that optimize classical ML algorithms, which returns one model per configuration, and compute the Pareto front a-posteriori.
On the other hand, there are others that leverage specific MO implementations of ML algorithms (MO-ML) that return a Pareto front of models.

\subsubsection{Optimizing standard-ML algorithms}

\textcolor{orange}{TODO}

\subsubsection{Optimizing MO-ML algorithms}

MO-ML algorithms return the complete Pareto front of models instead of a single model.
Formally, the signature of an algorithm changes to $A: \mathbb{D} \times \vec{\Lambda} \rightarrow 2^{\altmathcal{H}}$. As a consequence, the evaluation of each hyperparameter configuration of such an MO-ML algorithm also involves quantifying the quality of a Pareto front of models instead of a single model returned by the algorithm, yielding a much more difficult problem.
As such, the signature of our loss function also needs to change to $\altmathcal{L}: 2^{\altmathcal{H}} \times \mathbb{D} \rightarrow \mathbb{R}$ and thus can no longer be instantiated with simple loss functions such as accuracy.

One way of assessing the quality of a Pareto front and thus instantiating the HPO loss function $\altmathcal{L}$ in this case, are so-called Pareto front quality indicators~\citep{audet-ejor21}.
These can be categorized into so-called external and internal indicators where external indicators measure the convergence of a Pareto front to the optimal one, i.e., the one that the user prefers.
Yet, in real-case problems, computing the entire Pareto front space is unfeasible and there is no way to describe the desiderata beforehand.
In contrast, internal indicators assess the Pareto front quality by measuring specific characteristics.
A very common indicator is called hypervolume \citep{zitzler1999multiobjective} quantifying the volume occupied by the Pareto front w.r.t. a reference point.
Other indicators evaluate factors such as uniformity of solution distribution (e.g., spacing indicator \citep{schott1995fault}), range of values covered by the Pareto front (e.g., maximum spread indicator \citep{zitzler2000comparison}), or proximity to specific threshold points (e.g., R2 indicator \citep{hansen1994evaluating}).

Once a custom loss function tailored to Pareto front indicators is employed, the established single-objective optimization approaches remain applicable for optimizing MO-ML algorithms.


\section{Machine Learning Pipelines}

% Flow
% - transformation as algorithms a tutti gli effetti
% - take a dataset in input to perform a training process, affected by some hyperparameters as well, and outputs a model that can perform a certain task.
% - In this case, the nature of the task is not the goal of the analysis (e.g., classification) but rather increasing the quality of the original dataset.
% - For such a reason, the trained model maps an input dataset $\altmathcal{D} \in \mathbb{D}$ into a new one and $\altmathcal{D}' \in \mathbb{D}$.
As in the CRISP-DM model, the learning process not only involves algorithm training -- i.e., modelling -- but also data preparation, better known as data pre-processing.
Data pre-processing transformations are applied to improve data quality and to ease algorithms in achieving good performance.
% Different transformations are concatenated in pipelines to shape the data for the ML algorithm.
In practice, transformations are proper ML algorithms that (i) take a dataset in input, (ii) perform a training process, (iii) expose hyperparameters, and (iv) output a model that can perform a certain (pre-processing) task.
In this case, the nature of such a task is not the ultimate goal of the analysis (e.g., classification) but, rather, altering the original dataset (e.g., feature engineering) for a better result.
% In practice, for transformations, we do not seek models that map data items to predictions, i.e., $X \rightarrow Y$.
% Instead, the model signature of a pre-processing transformation changes to $X \rightarrow X'$, which allows to concatenate transformations in pipelines.
% This allows to concatenate transformations in pipelines. and shape the data according to the ML algorithm at hand (e.g., Decision Tree \Cref{ex:decision_tree}), which performs the ultimate task (e.g., classification).

\begin{definition}[Data Pre-processing Transformation]
    A \emph{transformation} $T$ is a particular case of an ML algorithm, in which the corresponding model -- after training -- maps an input dataset $\altmathcal{D} \in \mathbb{D}$ into a new one $\altmathcal{D}' \in \mathbb{D}'$.
    As an ML algorithm, its output is affected by hyperparameters $\lambda_1, \dots, \lambda_M$, with corresponding hyperparameter spaces $\Lambda_1, \dots, \Lambda_M$.
    A configuration is sampled from the cartesian product $\pmb{\Lambda} = \Lambda_1 \times \ldots \times \Lambda_M$.
    % Given a dataset $\altmathcal{D} \in \mathbb{D}$ and a hyperparameter configuration $\pmb{\lambda}$ sampled from the space $\pmb{\Lambda}$, a transformation $T(\altmathcal{D}, \pmb{\lambda})$ produces a dataset $\altmathcal{D}' \in \mathbb{D}$.
    % \begin{equation*}\label{eq:transformation}
    %     T: \mathbb{D} \times \pmb{\Lambda} \rightarrow \mathbb{D}
    % \end{equation*}
\end{definition}

As with ML algorithms, there exists a plethora of different alternatives for implementing transformations with a certain pre-processing task.
Besides, given the need to alter the dataset w.r.t. different pre-processing tasks, transformations are subsequentially applied in steps.

% As with ML algorithms, there exists a plethora of different alternatives for implementing transformations with a certain pre-processing task.
% Besides, given the need to alter the dataset under multiple perspectives, transformations are subsequentially applied in pipelines.
% Given the need to alter the dataset under multiple perspectives, transformations are subsequentially concatenated in pipelines.
% Specifically, a data pre-processing pipeline -- or data pipeline for short -- consists of different steps.
% , each of them implemented within a set of alternative transformations that share the same goal (i.e., pre-processing task).

\begin{definition}[Data Pre-processing Step]
    A data pre-processing step $S$ is a set of alternative transformations with the same pre-processing task.
    Hence, the instantiation of the step is a sample from the disjoint union of the transformations search spaces: given $T_1, \dots, T_N$ with corresponding hyperparameter spaces $\pmb{\Lambda}_1, \dots, \pmb{\Lambda}_N$, the step search space is defined as  $\pmb{\Lambda}  = \pmb{\Lambda} \cupdot \dots \cupdot \pmb{\Lambda}_N$.
\end{definition}

In the literature, there are data pre-processing steps that can be applied to both supervised and unsupervised learning because either do not leverage the ground truth intrinsically or there exists implementations that approximate the outcome as if it has not been provided.
\begin{itemize}
    \item \textit{Encoding} transforms categorical features in the form of strings into numerical representations.
    \item \textit{Normalization} scales continuous features such that their values fall in the same range.
    \item \textit{Discretization} transforms continuous features into categorical ones.
    \item \textit{Imputation} fills possible missing values among the instances.
    \item \textit{Feature Engineering} defines the most relevant features for the case at hand.
\end{itemize}

Besides, there are pre-processing steps that are aligned solely with respect to the analysis at hand.
\begin{itemize}
    \item \textit{Rebalancing}, in supervised learning, adjusts the class distribution of a dataset (i.e., so that each class is equally represented among the instances).
    \item \textit{Outlier Removal}, in unsupervised learning, drops any instances that can be considered noise because deviating (to a certain degree) from the overall distribution.
\end{itemize}

\begin{example}[Feature Engineering Step and Transformations in Supervised Learning]
    A feature engineering step reduces the dimensionality of the input dataset, mapping instances represented in $\altmathcal{X}$ features into a new set of features  $\altmathcal{X}'$ s.t.  $|\altmathcal{X}'| < |\altmathcal{X}|$.
    According to the specific implementation, transformations in this step either select the most relevant features (i.e., $\altmathcal{X}' \subset \altmathcal{X}$; e.g., ANOVA f-value test \cite{}) or create a new latent cartesian space (i.e., $X'_{i} \subset \altmathcal{X}' = \sum_{j}^{|\altmathcal{X}|} w_j \cdot X_j$ where $w_j \in \pmb{w}$ is a weight vector; e.g., PCA \cite{}).
\end{example}

Finally, steps are concatenated with a specific order in data pre-processing pipelines---or data pipelines in short.

\begin{definition}[Data Pre-processing Pipeline]
    Given a set of pre-processing steps $\altmathcal{S} = \{S_1,\dots, S_{K}\}$, a data pre-processing pipeline $P$ is a concatenation of the steps in $\altmathcal{S}$, with a specific order.
    Hence, the instantiation of the pipeline is a sample from the cartesian product of the steps search spaces: given $S_1, \dots, S_K$ with corresponding hyperparameter spaces $\pmb{\Lambda}_1, \dots, \pmb{\Lambda}_K$, the pipeline search space is defined as  $\pmb{\Lambda}  = \pmb{\Lambda} \times \dots \times \pmb{\Lambda}_K$.
\end{definition}

Given a set of pre-processing steps, with corresponding transformations and hyperparameter spaces, the best pipeline for the ML algorithm at hand is chosen from the disjoint union of all partial permutations of the pre-preprocessing steps.
We are hence provided with a data pipeline selection and optimization (DPSO) problem \cite{} and, as CASH, it can be reformulated as a single combined hierarchical hyperparameter optimization problem.

\begin{definition}[Data Pipeline Selection and Optimization (DPSO)]
    Given a set of data pre-processing steps $\altmathcal{S} = \{S_1, \dots, S_K\}$, with associated hyperparameter spaces  $\pmb{\Lambda}_1, \dots, \pmb{\Lambda}_K$, we define the hierarchical space $\pmb{\Lambda} = \pmb{\Lambda_1} \cup \dots \cup \pmb{\Lambda_K} \cup \{\lambda_r\}$, where $\lambda_r$ is a new root-level hyperparameter that selects the order of the steps---hence, the pipeline $P$ that should be instantiated. Then, the goal is to find the pipeline and corresponding configuration $P^{\star}_{\pmb{\lambda}^{\star}} \in \pmb{\Lambda}$ that, when used with the algorithm at hand $A$, leads to the optimal loss function.
    \begin{equation*}
        P^{\star}_{\pmb{\lambda}^{\star}} \in \argmin_{P_{\pmb{\lambda}} \in \pmb{\Lambda}} \altmathcal{L}(A(P_{\pmb{\lambda}}(\altmathcal{D}_{train})), \altmathcal{D}_{valid})
    \end{equation*}
    with $\altmathcal{D}_{train}$ and $\altmathcal{D}_{valid}$ representing two different splits in supervised learning, and corresponding to the same original dataset $\altmathcal{D}$ otherwise.
\end{definition}

Despite DPSO can be approached with the aforementioned optimization techniques for which CASH is addressed, different policies can be adopted to optimize the whole ML pipeline, i.e., the concatenation of a data pipeline with an ML algorithm.

\paragraph{Split policy}
The budget is split between $T1$ and $T2$, allocated respectively for the data pipeline configuration search and the algorithm hyperparameter tuning.
In the first phase, the optimizer (e.g., BO) is used during $T1$ to build the data pipeline.
During the second phase, the optimizer is used to configure the algorithm during $T2$.

\paragraph{Iterative Policy}
Both the data pipeline and the ML algorithm alternate during a short runtime.
The best configuration found during the previous optimization is reused during the next one, iteratively until the total budget has expired.
It is relatively fast for the optimizer to find a better pipeline configuration than the baseline but then stagnates.
Therefore, the time would be better allocated to the search for a better algorithm configuration.
If the data pipeline is modified, the data used to train the algorithm changes.
Those variations might help during the hyperparameter tuning to explore a new region of the search space compared to the previous iteration.

\paragraph{Adaptive Policy}
This policy reuses the iterative policy.
However, the time allocation is not fixed per iteration.
If during an iteration the loss improves, the time allocated to the next iteration of the same type (data pipeline or algorithm configuration) is multiplied by two.
Conversely, if after two iterations of the same type, the score is not improved, the allocated time for this type of iteration is divided by two.

\paragraph{Joint Policy}
This policy simply uses the union of both search spaces.
In other words, it is equivalent to what is usually done in practice by current optimizers, i.e., searching over the whole space of ML pipelines.

% This allows to concatenate transformations in pipelines and shape the data according to the ML algorithm at hand (e.g., Decision Tree \Cref{ex:decision_tree}), which performs the ultimate task (e.g., classification).



% \begin{example}[Feature Engineering Transformation]
%     A feature engineering transformation reduces the dimensionality of the input dataset, mapping instances represented in $\altmathcal{X}$ features into a new set of features $\altmathcal{X}'$ such that $|\altmathcal{X}'| < |\altmathcal{X}|$.
%     According to the specific implementation, transformations either select the most relevant features (i.e., $\altmathcal{X}' \subset \altmathcal{X}$; e.g., ANOVA f-value test \cite{}) or create a new latent Cartesian space (i.e., by performing a linear transformation using a weight vector $\pmb{w}$; e.g., PCA \cite{}).

%     Specifically, the linear transformation of a data point $\pmb{x}_i$ is given by:
%     \begin{equation*}
%         \pmb{x}_i' = \pmb{w}^T \pmb{x}_i = w_1x_{i1} + w_2x_{i2} + \ldots + w_px_{ip}
%     \end{equation*}
%     where $\pmb{w} = (w_1, w_2, \ldots, w_p)$ is the weight vector, and $\pmb{x}_i = (x_{i1}, x_{i2}, \ldots, x_{ip})$ is the original feature vector of data point $d_i$.
% \end{example}


% \begin{definition}[Linear Transformation]\label{def:linear-transformation}
%     A \textit{linear transformation} \(L\) on the dataset \(\altmathcal{D}\) is a mapping that operates on each data point \(d_i\) by creating a linear combination of its features. Let \(\pmb{w} = (w_1, w_2, \ldots, w_p)\) be a vector of weights and \(\pmb{x}_i = (x_{i1}, x_{i2}, \ldots, x_{ip})\) be the feature vector of data point \(d_i\). The linear transformation is defined as follows:
%     \begin{equation*}\label{eq:linear-transformation}
%         L(\pmb{x}_i) = \pmb{w}^T \pmb{x}_i = w_1x_{i1} + w_2x_{i2} + \ldots + w_px_{ip}
%     \end{equation*}
%     The transformed dataset \(\altmathcal{D}'\) is obtained by applying the linear transformation \(L\) to each data point \(d_i\) in \(\altmathcal{D}\):
%     \begin{equation*}\label{eq:transformed-dataset}
%         \altmathcal{D}' = \left\lbrace\,
%         (d_i')_{i=1}^N \quad \begin{array}{|@{}l@{}l@{\quad}l@{}}
%             \quad d_i' = (\pmb{x}_i', y_i) & \in \mathbb{D}' \subset \mathbb{R} \times \altmathcal{Y} & \text{{if }} T = \text{{supervised}} \\
%             \quad d_i' = \pmb{x}_i' & \in \mathbb{D}' \subset \mathbb{R} & \text{{if }} T = \text{{unsupervised}}
%         \end{array}
%         \right\rbrace
%     \end{equation*}
%     where \(\pmb{x}_i' = L(\pmb{x}_i)\) is the linearly transformed feature vector of data point \(d_i\).
% \end{definition}


% A Data (pre-processing) pipeline subsequentially concatenates transformations in steps, each one in charge of performing a specific pre-processing task (e.g., feature selection, normalization).
% Analogously to the AS problem, electing the right pipeline, i.e., which transformations and in which order, is subject to the ML algorithm at hand.
% Thus, analogously to typical AutoML problems, there are multiple transformations (e.g., PCA, select k best) for performing the same pre-processing task (e.g., feature selection), each with corresponding strengths and drawbacks.
% Pip


% Transformations (e.g., PCA, min-max scalarization) can be categorized depending on their scope classified according to the task performed
% Different algorithms usually need different pre-processing transformations, which may depend also on the dataset at hand.
% possibly concatenated in



% \begin{definition}[Machine Learning Algorithm]\label{def:algorithm}
%     Given the hyperparameters $\lambda_1, \dots, \lambda_M$, with corresponding spaces $\Lambda_1, \dots, \Lambda_M$, we refer with $\pmb{\lambda}$ to a hyperpaprameter configuration sampled from the space $\pmb{\Lambda} = \Lambda_1 \times \dots \times \Lambda_M$.
%     Given a dataset $\altmathcal{D} \in \mathbb{D}$ and a hyperparameter configuration $\pmb{\lambda} \in \pmb{\Lambda}$, an algorithm $A(\altmathcal{D}, \pmb{\lambda})$  provides a model from the model space $\mathbb{H}$.
%     \begin{equation*}\label{eq:algorithm}
%         A: \mathbb{D} \times \pmb{\Lambda} \rightarrow \mathbb{H}
%     \end{equation*}
%     For brevity, we refer to $A(\altmathcal{D}, \pmb{\lambda})$ with the notation $A_{\pmb{\lambda}}(\altmathcal{D})$, and $H_{\pmb{\lambda}}$ for the resulting trained model.
%     % For conciseness matter, we write $A_{\pmb{\lambda}}$ for $A(\altmathcal{D}, \pmb{\lambda})$ and refer to $H_{\pmb{\lambda}}$ as the corresponding model.
% \end{definition}



% Selecting the best pipeline can be
% Similarly to hyperparameter tuning of ML algorithms, we have to deal with an optimization problem.


