\section{Related work}
\label{sec:relatedwork}
A lot of ongoing research aims at addressing the problem of providing user assistance for the data analytics process.
Specifically, they can be classified into three main categories~\cite{elshawi2019automated}: distributed, cloud-based, and centralized. The first two try to address the problem of Big Data. Thus, clusters of several machines are employed to distribute the workload. On the contrary, this is not a fundamental requirement for centralized solutions. Indeed, the overhead of using a cluster is not worth for relatively small datasets. 
Since our work belongs to the category of centralized solutions, in the following, we provide examples of them.

As already mentioned before, the data analytics process consists of different steps. In general, there is a trend to develop (semi) automatic systems that assist the user in one or many steps altogether. At the beginning, the focus was to provide support exclusively for the learning step (i.e., the CASH problem). Recently however, the direction has shifted towards designing systems that additionally or specifically provide user assistance in the data pre-processing step (i.e., the DPSO problem).

When it comes to data pre-processing, different works have tackled this problem from different perspectives. For instance, there are works that aim to apply pre-processing for the sake of guaranteeing data quality, or enabling data exchange, or even data integration. That is, they consider data pre-processing in isolation or apart from data analysis~\cite{Llunatic13VLDBEnd,BigDansing15SIGMOD,Katara15SIGMOD,Foofah17SIGMOD}. In this, and our related work however, we consider only the works that see pre-processing as an integral part of data analytics and hence apply it for the sake of improving the final result of the analysis.


Finally, there are works that aim at fully automating the data analytics process (i.e., automatically generate data analytics flows), which roughly translates to combining DPSO with CASH, where the border line between the latter two becomes blurry. Nevertheless, we tentatively group the works based on the type of the problem they aim to solve.

\subsection{DPSO}

In DPD \cite{Quemy20InfSystems}, the DPSO problem, as we use it in this work, is formally defined. Authors demonstrate the impact of optimizing the pre-processing pipeline, but considering only a single fixed pipeline prototype. %and only a few datasets. 
However, as we have already seen (Section~
\ref{sec:eval-universal-pipeline}), a single fixed prototype cannot perform best for every dataset. Therefore, we build on top of~\cite{Quemy20InfSystems}, and instead of relying on a fixed prototype, we define a method to generate the right pipeline prototypes to be optimized.

In PRESISTANT~\cite{presistant18CSI,presistant18CAISE,presistant19DKE}, we tackled the problem of recommending pre-processing operators to the non-expert data analyst. The goal, and at the same time the challenge was to identify the pre-processing operators, and rank them in advance, based on their potential impact to the final analysis. However, we did not consider pre-processing pipelines, but only single transformations, expecting that the analyst applies the process iteratively. In this work, we consider sets of transformations and thus study the impact of combining transformations into a pipeline. 

In ActiveClean \cite{ActiveClean16PVLDB}, authors define an algorithm that aims at prioritizing the cleaning of records that are more likely to affect the results of the statistical modeling problems, assuming that the latter belong to the class of convex loss models (i.e., linear regression and SVMs). Hence, instead of recommending the transformations to be applied, the system recommends the subset of data which needs to be cleaned at a given point. The type of pre-processing to be applied is left to the user, assuming that the user is an expert. 

In Learn2Clean~\cite{Berti19WWW}, based on a reinforcement learning technique, for a given dataset, and an ML model, an optimal sequence of operators for pre-processing the data is generated,
such that the quality of the ML model is maximized. Here, similarly to~\cite{Quemy20InfSystems}, the pipeline prototype is fixed in advance. Our work is a step further in that we help to choose the right pipeline prototype, instead of fixing it in advance. 

In Alpine Meadow~\cite{Shang19SIGMOD}, authors follow a similar approach to ours in that they define two steps for the pre-processing phase. One, the so called \textit{logical pipeline plan}, which is roughly equivalent to the \textit{pipeline prototypes} defined in this work, and the second the \textit{physical pipeline plan} which translates to \textit{pipelines} used in this work. The physical plan is generated through a combination of Bayesian optimization, meta-learning, and multi-armed bandits. For the logical plans, they rely on rules but without clear evidence on how they are generated. Moreover, it is not clear whether the logical plan is fixed as in~\cite{Quemy20InfSystems} and if some further adjustment from the user is required. 

\subsection{CASH} The task in solving the CASH problem is to automatically find an optimized instantiation for the hyper-parameters of the ML algorithm. Most of the works use Bayesian optimization methods to tune and optimize them~\cite{Feurer15AutoSklearn,Thornton13AutoWeka,Olson16Tpot}. Since Bayesian optimization is randomized, meta-learning has been used to find a good seed for the search~\cite{Feurer15AAAI}. Most of these works however, only minimally consider the data pre-processing step.
%\besim{Is this sentence correct now? If not just remove it.}
Auto-WEKA~\cite{Thornton13AutoWeka}, based on the Java machine learning library Weka, is the pioneer of the field.
The authors formalized the problem of algorithm selection and their associated hyper-parameter optimization, and solved it in a combined search space. 
Sequential Model-based Algorithm Configuration (SMAC) is used to explore the large search space.

Autostacker~\cite{chen2018autostacker} combines a hierarchical stacking architecture and an evolutionary algorithm (EA).
Stacking is an ensemble method that involves the concatenation of several classifiers, so that the later layers can learn the mistakes that classifiers in the previous layers make.
Even if it brings some benefits, this approach affects the search space: way larger than that of a single classifier.
In a nutshell, such concatenations are randomly generated and then optimized.
The one that achieves the higher predictive accuracy is chosen.
Rather than Bayesian Optimization, to find suitable hyper-parameters, the authors utilize a basic Evolutionary Algorithm.

OBoe~\cite{yang2019oboe} exploits collaborative filtering for AutoML, choosing models that have performed well on similar datasets.
It collects a large number of datasets and applies different ML algorithms (with different hyper-parameters configurations). 
In this way, a matrix of cross-validated errors is built.
Common approaches typically compute dataset meta-features and use them to predict the error of a particular machine learning model, but OBoe works exactly the other way around.
PCA is applied on such a matrix in order to find latent meta-features.
Given a new dataset, some basic algorithms are applied to infer a feature vector (i.e., the value of the latent meta-features).
Finally, the feature vector is leveraged to estimate the cross-validated error of more complex algorithms.

%\besim{We can add more works here if needed...}
%Google vizier: A service for black-box optimization

%Random search for hyper-parameter optimization ??

%Towards interactive curation \& automatic tuning of mlpipelines

%A review of automatic selection methods for machinelearning algorithms and hyper-parameter values.
 %refer to systems that try to automatically optimize the hyperparameters of ML algorithms. The goal is to automatically generate workflows or machine learning pipelines that give optimal results for the task at hand. Typically, Bayesian optimization methods are used to tune and optimize the hyperparameters.    

%Hellix aims to accelerate the iterative MLmodel training with responsive user feedback [42]. Vizdom[10] provides a unique pen-and-touch interface for the userto easily construct ML workflows and interactively refinethe analytics/ML pipelines. Most industry cloud ML services,such as TensorFlow [2], Amazon SageMaker and Azure Ma-chine Learning [27], fall into this category, in that they pro-vide fully-managed environment for ML applications. Unlikesystems, the focus is not automated end-to-end pipeline cura-tion; the services provide programmable APIs or web-basedinterface for ML workflow construction and managed com-puting resources for the deployment.
 
\subsection{DPSO + CASH}
Auto-sklearn~\cite{Feurer15AutoSklearn} is based on the popular Python library scikit-learn.
The authors, inspired by Auto-Weka, address the problem with the Sequential Model-based Algorithm Configuration (SMAC).
Furthermore, they improve the approach by adding a meta-learning phase at the beginning (to warm-start the Bayesian Optimization) and an ensemble technique at the end (to suggests multi-classifiers).
Such a system considers pre-processing transformations to generate end-to-end analytic pipelines. 
Yet, they consider a small set of transformations and also consider a single fixed pipeline prototype. 
Our work in a way is complementary to this, since instead of a priori fixing the prototype, we can construct a potentially optimal one (or a set), and then provide it to the tool for it to be instantiated and further optimized.
TPOT~\cite{Olson16Tpot} is a tree-based pipeline optimization tool using genetic programming while requiring little to no expertise from the user. In TPOT however, they only consider one transformation inside the optimization process (i.e., Feature Engineering).

%Based on the work of~\cite{Nicolo18NIPS}, Microsoft developed an AutoML tool via Azure. They build predictive ML pipelines combining collaborative filtering and Bayesian Optimization. In particular they model the search space as probability distribution defined by a Probabilistic Matrix Factorization and then use expected improvement as acquisition function to choose the most promising pipeline.

ML-Plan~\cite{mohr2018ml} uses hierarchical planning, a particular form of AI planning, to propose a solution to both the pre-processing and the modeling phases. 
As in context-free grammars, there are complex tasks (non-terminal symbols) that are derived as long as primitive tasks (terminal symbols) are not obtained.
Typically, standard graph search algorithms (e.g., depth-first search, best-first search, etc.) are employed to solve such problems.
ML-Plan successively creates solutions in a global search instead of changing given solutions in a local search. However, due to the problem constraints, they adopt a randomized best-first search, randomly choosing the solution path.

AutoBazaar~\cite{AutoBazaar} is a Python open-source tool.
Like in ML-Plan~\cite{mohr2018ml}, both pre-processing and modeling phases are covered.
Here the last step of a prototype is the machine learning algorithm.
The approach involves two different steps.
Firstly, a \textit{catalog} proposes a collection of prototypes (with an ML algorithm as last step) based on the task and the dataset itself.
Secondly, the optimization process starts tuning the prototypes until either the time budget is expired or the prototypes are all optimized.
In particular, a \textit{selector} and a \textit{tuner} work in synergy.
The former decides which prototype should be optimized next. 
Such a task is treated as a multi-armed bandit problem.
As to the tuner, Bayesian Optimization is chosen.
At the end, the prototype that achieved the higher predictive accuracy is elected.
However, AutoBazaar strictly depends on the catalog.
Such a component memorizes all the possible primitives and supported tasks.
The prototypes are hard-coded for each task.
Thus, it is neither flexible nor maintainable.
If a task is not implemented, the approach cannot suggest a solution.

To summarize, full automation of data analytics has been the ultimate goal of many research works. Yet, such an automation has shown to be computationally expensive, mainly due to the search space involved (i.e., pre-processing and mining operators). Therefore, the usability of these approaches in realistic scenarios is sometimes limited. Our approach of finding a set of effective pipeline prototypes can be seen as complementary to these solutions, since it helps in pruning the large space and guiding the search, hence reducing their cost.