% ******************************* PhD Thesis Template **************************
% Please have a look at the README.md file for info on how to use the template

\documentclass[a4paper,12pt,times,numbered,print,index]{Classes/PhDThesisPSnPDF}

\usepackage[T1]{fontenc}

% ******************************************************************************
% ******************************* Class Options ********************************
% *********************** See README for more details **************************
% ******************************************************************************

% `a4paper'(The University of Cambridge PhD thesis guidelines recommends a page
% size a4 - default option) or `a5paper': A5 Paper size is also allowed as per
% the Cambridge University Engineering Department guidelines for PhD thesis
%
% `11pt' or `12pt'(default): Font Size 10pt is NOT recommended by the University
% guidelines
%
% `oneside' or `twoside'(default): Printing double side (twoside) or single
% side.
%
% `print': Use `print' for print version with appropriate margins and page
% layout. Leaving the options field blank will activate Online version.
%
% `index': For index at the end of the thesis
%
% `draftclassic': For draft mode without loading any images (same as draft in book)
%
% `draft': Special draft mode with line numbers, images, and water mark with
% timestamp and custom text. Position of the text can also be modified.
%
% `abstract': To generate only the title page and abstract page with
% dissertation title and name, to submit to the Student Registry
%
% `chapter`: This option enables only the specified chapter and it's references
%  Useful for review and corrections.
%
% ************************* Custom Page Margins ********************************
%
% `custommargin`: Use `custommargin' in options to activate custom page margins,
% which can be defined in the preamble.tex. Custom margin will override
% print/online margin setup.
%
% *********************** Choosing the Fonts in Class Options ******************
%
% `times' : Times font with math support. (The Cambridge University guidelines
% recommend using times)
%
% `fourier': Utopia Font with Fourier Math font (Font has to be installed)
%            It's a free font.
%
% `customfont': Use `customfont' option in the document class and load the
% package in the preamble.tex
%
% default or leave empty: `Latin Modern' font will be loaded.
%
% ********************** Choosing the Bibliography style ***********************
%
% `authoryear': For author-year citation eg., Krishna (2013)
%
% `numbered': (Default Option) For numbered and sorted citation e.g., [1,5,2]
%
% `custombib': Define your own bibliography style in the `preamble.tex' file.
%              `\RequirePackage[square, sort, numbers, authoryear]{natbib}'.
%              This can be also used to load biblatex instead of natbib
%              (See Preamble)
%
% **************************** Choosing the Page Style *************************
%
% `default (leave empty)': For Page Numbers in Header (Left Even, Right Odd) and
% Chapter Name in Header (Right Even) and Section Name (Left Odd). Blank Footer.
%
% `PageStyleI': Chapter Name next & Page Number on Even Side (Left Even).
% Section Name & Page Number in Header on Odd Side (Right Odd). Footer is empty.
%
% `PageStyleII': Chapter Name on Even Side (Left Even) in Header. Section Number
% and Section Name in Header on Odd Side (Right Odd). Page numbering in footer

% Uncomment to change page style
%\pagestyle{PageStyleII}

% ********************************** Preamble **********************************
% Preamble: Contains packages and user-defined commands and settings
\input{global/packages}

% ************************ Thesis Information & Meta-data **********************
% Thesis title and author information, reference file for biblatex
% \input{fronte}

% ***************************** Abstract Separate ******************************
% To printout only the titlepage and the abstract with the PhD title and the
% author name for submission to the Student Registry, use the `abstract' option in
% the document class.

\ifdefineAbstract
 \pagestyle{empty}
 \includeonly{Declaration/declaration, Abstract/abstract}
\fi

% ***************************** Chapter Mode ***********************************
% The chapter mode allows user to only print particular chapters with references
% Title, Contents, Frontmatter are disabled by default
% Useful option to review a particular chapter or to send it to supervisor.
% To use choose `chapter' option in the document class

\ifdefineChapter
 \includeonly{Chapter3/chapter3}
\fi

% ******************************** Front Matter ********************************
\begin{document}

\frontmatter
\include{global/fronte}
% \includepdf[pages=-]{global/frontespizio.pdf}
\newpage
% \includepdf[pages=-]{global/researchactivity.pdf}
\include{global/dedication}
% \include{global/declaration}
\include{global/quote}
\include{global/acknowledgement}
\include{global/abstract}
\tableofcontents
% \listoffigures
% \listoftables
% \printnomenclature%[3em]

% ******************************** Main Matter *********************************
\mainmatter
\sloppy
% \include{global/preamble}

\chapter*{Introduction}

Artificial intelligence (AI) is currently the buzzword on everybody's lips.
Riding the wave of recent groundbreaking achievements, from self-driving cars \citep{} to intelligent chatbots \citep{}, AI is transforming industries and reshaping our daily lives.
Several interpretations and definitions have been provided over the years, yet the seminal perspective given by the Turing test \citep{turing1980computing} is still one worth mentioning.
\begin{definition}[Artificial Intelligence \citep{turing1980computing}]
A machine that shows intelligence indistinguishable from that of human beings is qualified to be labeled as artificial intelligence.
\end{definition}
This translates into understanding what falls under the umbrella of \textit{intelligence}, which is defined differently across the research areas as reasoning, planning, and learning.

\begin{definition}[Machine Learning \citep{samuel2000some}]
A machine with the ability to learn without being explicitly programmed.
\end{definition}
Building upon the notion of intelligence as learning, i.e. Machine Learning (ML), emerged as the pinnacle of AI due to its disruptive advancements.
At its core, ML aims at addressing problems for which the development of algorithms by humans is not feasible, because the algorithm itself is either not known or cost-prohibitive.
Examples include face recognition, fraud detection, sale forecasting, and object ranking. 
The problems are solved instead by letting the algorithms (e.g., neural networks \cite{}) \textit{discover their own solutions}: they perform a training process atop a sample of historical data, borrowing techniques from disciplines such as numerical analysis, statistics, and information theory.
The training process consists of fitting internal parameters (e.g., weights and bias) and providing ML practitioners with a model, which is ready to ingest new data and tackle the problem at hand.

There exists a plethora of different algorithms solving the same problem with different strengths and weaknesses, confirming theoretical results proving that there is \textit{no silver bullet} \cite{kerschke2019automated}---no algorithm dominates all others in all respects.
Besides, algorithms often expose some hyperparameters controlling the learning behavior (e.g., learning rate). 
To unleash the full potential of ML, practitioners have to carefully tune such hyperparameters but get easily overwhelmed by the showcased problem of combined algorithm selection and hyperparameter (CASH) optimization.

Automated machine learning (AutoML) demonstrates to play a crucial role in this landscape by tackling the CASH problem and, going beyond, by handling ever-larger search spaces in surprisingly small time budgets \cite{}.
Remarkable milestones include bayesian optimization (BO) to explore promising configurations based on prior evaluations,
meta-learning (i.e., learning atop learning) to warm-start BO (i.e., to boost the convergence process) by suggesting configurations that worked well in previous similar cases, and multi-fidelity methods to partially evaluate time-consuming configurations.
Besides, off-the-shelf solutions \cite{} are provided to tune entire ML pipelines, achieving -- in some cases -- higher performance than experts.

By lowering the barrier of access, AutoML emerged as promising for democratization of AI, i.e. making it accessible to both experts and non-experts alike.
Yet, when it comes to real-case scenarios, the journey of learning is riddled with challenges, ranging from the need for human intervention to mitigate harnessing, to the need for physical simulators.
% and heterogeneity of the data to constraints that may apply to the problem, from need for domain knowledge to .
While \Cref{chap:background} provides the necessary background, the remaining of the thesis investigates these challenges evolving into two parts.




\paragraph{Part I: Human-centered AutoML}

The original promise of AutoML was to automate certain ML tasks to a significant extent, thereby democratizing it and enabling non-experts to apply it in their respective domains.
However, despite their success, many current AutoML tools were not built around the user but rather around algorithmic ideas.
The stacking of complex mechanisms on top of each other unavoidably led to a less understanding of the process by the user -- even ML experts -- and allowing for very limited interaction.
Parts of the community have hence pushed towards a more human-centered AutoML process aimed at complementing, instead of replacing, human intelligence.
% This becomes even more crucial nowadays
% Motivated by ethical concerns and social bias issues arising at each step of the ML process, this becomes imperative in such a mitigating call nowadays.
Besides, motivated by ethical concerns and social bias issues arising at each step of the ML process, this approach becomes even imperative in such a mitigating call nowadays.
By placing the user back in the loop, it would be possible to revise and supervise the entire process, ensuring fairness, transparency, and ethical compliance.

In this part, we focus on the following contributions.
\begin{itemize}
    \item Providing the AutoML formalization to explicitly deal with thorough pipelines i.e., concatenations of pre-processing transformations and ML algorithms.
    \item Devising human-centered solutions for the main categories of ML i.e., supervised and unsupervised learning. The former is provided with a ground truth to drive the learning towards a user-specified objective function, the latter is by nature exploratory -- does not benefit from any ground truth -- and optimizing an objective is even more challenging.
    \item Addressing multi-objective ML, i.e. optimization of more than one loss or objective function, by interactively learning user preferences and drive the optimization towards it.
    \item Showcasing potential challenges, opportunities, and risks of novel (human-centered) AutoML interfaces with large language models (LLMs), i.e. AI models trained on large corpora of text to understand and produce human-like answers.
\end{itemize}

We organize this part as follows.
\Cref{automl-chap:formalization} provides the comprehensive AutoML formalization for ML pipelines.
\Cref{automl-chap:supervised} and \Cref{automl-chap:unsupervised} address supervised and unsupervised learning respectively.
\Cref{automl-chap:moo} delves into multi-objective ML and, lastly, \Cref{automl-chap:llm} discusses the novel human-centered AutoML interfaces with LLMs.
% In this part, we devise human-centered AutoML solutions for the main categories of ML.
% While in \Cref{automl-chap:formalization} we provide the AutoML formalization to explicitly deal with thorough pipelines i.e., concatenations of pre-processing transformations and ML algorithms, \Cref{automl-chap:supervised} and \Cref{automl-chap:unsupervised} address supervised and unsupervised learning respectively.
% The former is provided with a ground truth to drive the learning towards a user-specified objective function, the latter is by nature exploratory -- does not benefit from any ground truth -- and optimizing an objective is even more challenging.
% Then, \Cref{automl-chap:moo} delves into multi-objective ML, i.e. optimization of more than one loss or objective function.
% Lastly, \Cref{automl-chap:llm} showcases the potential challenges, opportunities, and risks of novel (human-centered) AutoML interfaces with large language models, i.e. AI models trained on large corpora of text to understand and produce human-like answers.

\paragraph{Part II: Physics-coupled AutoML}

As application areas go, there are domains in which ML models are not yet widespread.
Usually related to earth observations, the deployed applications tend to be particularly high-impact, trying to mitigate challenges such as climate change by delivering (more) eco-friendly systems.
Examples include weather forecasts and crop life-cycle management.
Domain experts leverage their knowledge to tune numerical simulators -- which encode well-known physical laws -- and deliver forecasts and analyses.
However, this process faces several challenges.
% several issues jeopardize this process.
The non-existence of universal well-defined practices translates to several trials and errors, with domain experts manually configuring the simulator parameters until an acceptable solution is found.
Yet, as variables involved in physics phenomena are subject to constant change, such numerical solution must undergo periodic (re-)calibration.
Besides, with the torrent of remotely sensed data available today, opportunities to integrate real-time observations in predictive models have emerged that traditional methods are not equipped to handle.
This is where advances in AI can push forward to enhance our understanding, provide support and, if necessary, steer the course of resolving global concerns to a more favorable future.
For instance, ML models exhibit singular performance in adapting themselves to handle scenarios different from the one they were trained on (e.g., fine-tuning of network parameters) and AutoML can significantly impact in automating several stages (e.g., tuning simulators and ML models).
% At the same time, relying on fully-automated data-driven methods for problems as complex and impactful as these is doomed to result in biased and unreliable.
% This underscores the ongoing importance of domain experts in this endeavor, highlighting their crucial role, with AI serving as a powerful tool.
% This is why domain experts, as always, remain crucial in this process, and why AI should be there as a powerful tool that supports them in fully taking advantage of the new opportunities we are presented with.

In this part, we focus on precision farming, which plays a pivotal role in addressing water wastage and improving crop efficiency.
\Cref{precision-chap:background} provides the necessary background and formalization in precision farming.
\Cref{precision-chap:architecture} overviews the architecture of our big-data smart-irrigation platform.
\Cref{precision-chap:orchard} deepens on Orchard3D-Lab, a well-known crop and soil simulator enhanced with auto-tuning and data assimilation capabilities, i.e. ingestion of sensor data for more reliable estimations.
\Cref{precision-chap:pluto} and \Cref{precision-chap:forecasting} delve -- respectively -- into the modules of real-time soil moisture monitoring and forecasting.
Finally, \Cref{precision-chap:smart-irrigation} deepens the smart-irrigation algorithm.


\chapter{Background}
\label{chap:background}

Alan Turing defines the field of AI in cognitive terms \cite{turing1980computing}, raising the question of whether machines can show intelligence, and along this line Arthur Samuel defines ML \cite{samuel2000some}, building upon the notion of intelligence as learning.
A more formal definition is provided by Tom M. Mitchell \citep{mitchell1997machine}, who delineates the behavior of algorithms studied in machine learning and introduces the concepts of \textit{task} and \textit{experience}.
\begin{definition}[Machine Learning \citep{mitchell1997machine}]
    A computer program is said to learn in some class of tasks, with respect to a performance measure, if its performance improves with experience.
\end{definition}
Notably, the formalization of experience is contingent upon the task to address, which is crucial because determines what the ML algorithm can observe and, in turn, how it operates.
Specifically, the literature identifies three different ML tasks: \textit{supervised}, \textit{unsupervised}, and \textit{reinforcement learning}.

\paragraph{Supervised learning} This is the kind of learning studied in most current research in the ML field. The algorithm is provided with a sample of data describing scenarios that have been \textit{experienced} in the past along with the corresponding \textit{ground truth}---i.e., values that indicate the correct outcomes.
The goal consists of finding a \textit{function} that maps samples to outcomes so that the next unseen scenarios can be labeled correctly.


\paragraph{Unupervised learning}
Here, the samples come with no ground truth available.
The algorithm has to guess the outcome by uncovering hidden insights or patterns, investigating different partitionings, or estimating density distributions within the data.
Analogously, tasks can be further categorized.
\begin{itemize}
    \item \textbf{Clustering} or \textbf{cluster analysis} tasks, when the outcome can assume finite values and it refers to the group that better represents the instances e.g., identifying customer segments based on purchasing behavior.
    \item Otherwise, the nomenclature usually comes after the task semantic e.g., anomaly detection assigns an outlier score to each instance that deviates from the overall distribution.
\end{itemize}
% Unsupervised tasks are inherently exploratory in nature and are often employed in data mining to provide explanations for the given scenarios or even to summarize them.

\paragraph{Reinforcement Learning}
Standing on the edge of the ML landscape, it has been categorized as the learning that differentiates the most from ``classical'' tasks \cite{sutton2018reinforcement}.
Here the experience is not sampled and fed to the algorithm but rather comes from interactions with an environment.
The algorithm learns by making decisions and receiving feedback based on its actions, allowing it to adapt its behavior over time.\\

%TODO: Qua c'è da menzionare che infatti automatizzare RL, in letteratura, prende il nome di AutoRL mentre noi facciamo AutoML.
This thesis focuses on supervised and unsupervised learning.
% providing a formalization that covers both cases.
Particularly in this chapter, \Cref{automl-background-sec:ml} introduces the building blocks such as dataset, algorithm, and hyperparameter; while
% provides the necessary background in ML,
\Cref{automl-background-sec:automl} formalizes the problems tackled by automated machine learning (AutoML), up to the combined algorithm selection and hyperparameter optimization.
% along with the state-of-the-art techniques to solve them.
% and finally \Cref{automl-background-sec:formalization} extends the formalization towards a more operational level of ML, enabling the user to handle comprehensive ML pipelines---i.e., concatenations of steps that cover more than the sole learning process.
% Here we can also put something like " ... extends the formalization focusing on a more operational level of ML and methods that following this way enables to handle comprehensive ... "

\section{Machine Learning}\label{automl-background-sec:ml}

At the core of each and every task lies the data, serving as the essential fuel for the learning process because providing the means to observe the real world.
Specifically in ML, we refer to a dataset $\altmathcal{D}$ as a sample of data from which it is possible to learn that experience of interest.

\begin{definition}[Dataset]\label{def:dataset}
    A \textit{dataset} $\altmathcal{D}$ is a collection of data points $X$, with corresponding space $\altmathcal{X}$.
    According to the task $T$, the dataset may include the ground truth $Y$ within its corresponding space $\altmathcal{Y}$ (supervised) or not (unsupervised).
    \begin{equation*}\label{eq:dataset}
        \altmathcal{D} = \left\lbrace\,
        \begin{array}{@{}l@{}l@{\quad}l@{}}
            (\pmb{x}_i, y_i)_{i=1}^N & \in \mathbb{D} \subset \altmathcal{X} \times \altmathcal{Y} & \text{{if }} T = \text{{supervised}} \\
            (\pmb{x}_i)_{i=1}^N & \in \mathbb{D} \subset \altmathcal{X} & \text{{if }} T = \text{{unsupervised}}
        \end{array}
        \right.
    \end{equation*}
\end{definition}

Intuitively, a dataset resembles a structured table where each row represents a unique instance $x \in X$ characterized by specific features in their domain $\altmathcal{X}$.
The features, denoted by the columns, describe the diverse factors or characteristics influencing a particular outcome $Y$ in its domain $\altmathcal{Y}$.
While in supervised learning the ground truth is provided with the aim of understanding the hidden relationships between features and corresponding outcomes, in supervised tasks, the most appropriate outcome has to be found by discovering patterns within the data.

\begin{example}[Iris dataset \cite{iris}]\label{ex:dataset}
    The iris dataset contains $150$ instances of flowers ($X$) under four features in cm ($\altmathcal{X} \subset \mathbb{R}^4$): sepal length, sepal width, petal length, and petal width.
    The data have been collected to quantify the morphologic variation within the Iris species ($Y$), which can assume the following $3$ classes ($\altmathcal{Y}$): Iris setosa, Iris virginica and Iris versicolor.
    \begin{table}[!h]
        \centering
        \begin{tabular}{llll|l}
            \hline
            Sepal length & Sepal width & Petal length & Petal width & Class \\ \hline
            % $5.1$ & $3.5$ & $1.4$ & $0.2$ & Iris-setosa \\
            $4.9$ & $3.0$ & $1.4$ & $0.2$ & Iris-setosa \\
            % $7.0$ & $3.2$ & $4.7$ & $1.4$ & Iris-versicolor \\
            $6.4$ & $3.2$ & $4.5$ & $1.5$ & Iris-versicolor \\
            % $6.3$ & $3.3$ & $6.0$ & $2.5$ & Iris-virginica \\
            $5.8$ & $2.7$ & $5.1$ & $1.9$ & Iris-virginica \\ \hline
            \
        \end{tabular}
        \caption{Example tuples from the Iris dataset.}
        \label{tbl:iris}
    \end{table}

    \noindent This dataset is also used in unsupervised learning tasks by discarding the class attribute, yet the data only contains two clusters with rather obvious separation.
\end{example}

% The aim is to discover the hidden relationships between inputs $X$ and outputs $Y$, hence learning a function $f: \altmathcal{X} \rightarrow \altmathcal{Y}$.
% Despite the task,
The ultimate goal is learning a function $f: \altmathcal{X} \rightarrow \altmathcal{Y}$.
A (machine) learning \textit{algorithm} $A$ leverages data points in $\altmathcal{D}$ to estimate such a function $f$, which is expressed via a vector of \textit{model parameters} $H$.
Most algorithms further expose hyperparameters $\lambda_1, \dots, \lambda_K$ that change the functioning of the algorithm itself.

\begin{definition}[Machine Learning Algorithm]\label{def:algorithm}
    Given a dataset $\altmathcal{D}$ and a hyperparameter space $\pmb{\Lambda}$, an algorithm $A$ provides a model $H$ from the model space $\altmathcal{H}$.
    \begin{equation*}\label{eq:algorithm}
        A: \mathbb{D} \times \pmb{\Lambda} \rightarrow \altmathcal{H}
    \end{equation*}
    Given the hyperparameters $\lambda_1, \dots, \lambda_M$, with corresponding spaces $\Lambda_1, \dots, \Lambda_M$, we refer with $\pmb{\lambda}$ to a hyperpaprameter configuration sampled from the space $\pmb{\Lambda} = \Lambda_1 \times \dots \times \Lambda_M$.
\end{definition}

The actual learning is performed via the so-called \textit{model training} or, due to the iterative process of adjusting internal parameters until convergence, \textit{model fitting}.
The quality of such a model heavily depends on the hyperparameter choices we made, hence we need loss functions that assess the quality of different configurations.
% How well a model performs depends heavily on the hyperparameter choices we make and loss functions assess the quality of such configurations.
The term loss typically refers to the error committed by the model, hence the lower the better; instead, when higher values are favored, we refer to quality metrics.
Quality metrics share the same signature as the losses.

\begin{definition}[Loss Function]\label{def:loss}
    Given a dataset $\altmathcal{D}$ and a model $H$, the loss function $\altmathcal{L}$ quantifies how well the given model performs on $\altmathcal{D}$.
    \begin{equation*}\label{eq:loss}
        \altmathcal{L}: \altmathcal{H} \times \mathbb{D} \rightarrow \mathbb{R}
    \end{equation*}
\end{definition}

In the following, we walk over the main characteristics and differences between the specific cases of supervised and unsupervised learning.

\subsection{Supervised Learning}

First of all, supervised learning can be further differentiated based on the nature of the desired outcome.
\begin{itemize}
    \item \textbf{Classification} tasks, when the outcome can assume finite values and it represents the class to which the instances belong e.g., in an image recognition use-case: ``dog'', ``cat'', ``car'', or ``tree''.
    \item \textbf{Regression} tasks, when the outcome is continuous e.g., predicting house prices, stock prices, or temperature.
\end{itemize}
This might alter how some algorithms internally work but, for our perspective, they are still designed to be fed with a labeled dataset and, through model training, to minimize the discrepancy between its predictions and actual outcomes.
The ultimate goal is to deploy a model that performs well on new data, hence achieving a good \textit{generalization}.
Yet, two undesired scenarios can occur.
\textit{Overfitting} takes place when the model is too complex and fits the training data too closely, not being able to generalize the acquired knowledge to unseen data and capturing noise rather than genuine patterns.
On the contrary, \textit{underfitting} happens when a model is too simplistic, failing to capture the underlying complexities.
Hyperparameters are crucial in striking the right balance.

\begin{example}[Decision Tree]
    The decision tree algorithm recursively splits instances based on their feature values, creating a hierarchical tree-like structure where each leaf represents a class or a prediction value---in classification and regression tasks respectively.
    % Notably, in its supervised nature, the tree can be built by minimizing the entropy of the leaves.
    We can control the complexity of such a tree by leveraging hyperparameters such as the maximum depth of the tree and the minimum number of instances in the leaves.
    In this case, a deeper tree with more splits may capture intricate patterns but is prone to overfitting, as to a shallower tree, it might generalize better but may overlook finer details.
\end{example}

Loss functions are inevitable when it comes to assess generalization performance but their application should also follow protocols that assure consistency and statistical reliability.
The most common protocol involves splitting the original dataset $\altmathcal{D}$ into two disjoint sets $\altmathcal{D}_\mathit{train}$ and $\altmathcal{D}_\mathit{valid}$, where the model is trained only based on $\altmathcal{D}_\mathit{train}$ but validated with $\altmathcal{L}$ on $\altmathcal{D}_\mathit{valid}$.
Yet, both model training and validation demand substantial amounts of data.
When the split is not feasible, a practical solution is the $k$-fold cross-validation technique.
This divides the dataset into $k$ folds and, for each fold, uses the corresponding subset of data for testing while employing the remaining $(k - 1)$ folds for training.

\begin{definition}[$K$-Fold Cross-Validation]
    The $k$-fold cross-validation provides a protocol to validate an algorithm $A$ with corresponding hyperparameter configuration $\lambda$ via a loss $\altmathcal{L}$ on a dataset $\altmathcal{D}$.
    \begin{equation*}
        \frac{1}{k}~\mathlarger{\sum}_{i=1}^{k} \altmathcal{L}(A(\altmathcal{D}_{train}^{(i)}, \lambda), \altmathcal{D}_{valid}^{(i)})
    \end{equation*}
    where $\altmathcal{D}_{train}^{(i)} = \altmathcal{D} \setminus \altmathcal{D}_{valid}^{(i)}$.
\end{definition}

Follows two examples of losses: misclassification error (\Cref{ex:misclassification}) and root mean square error (\Cref{ex:rmse}), for classification and regression tasks respectively.

\begin{example}[Misclassification Error]\label{ex:misclassification}
    Given the validation set $(x, y) \in D_\mathit{valid}$  and the model $H$ resulted from the training $A(D_\mathit{train}, \lambda)$, then the loss MIS computes the fraction of incorrect predictions made by $H$.
    \begin{equation*}
        MIS(H, \altmathcal{D}_\mathit{valid}) = \frac{1}{|D_\mathit{valid}|}~\mathlarger{\sum}_{(x, y) \in D_\mathit{valid}} (H(x) \ne y)
    \end{equation*}
    In classification tasks, the accuracy ($ACC = 1 - MIS$) computes the fraction of incorrect predictions and is often used as a quality metric.
\end{example}


\begin{example}[Root Mean Square Error]\label{ex:rmse}
    Given the validation set $(x, y) \in D_\mathit{valid}$  and the model $H$ resulted from the training $A(D_\mathit{train}, \lambda)$, then the loss RMSE computes the average difference between the predictions made by $H$ and the actual values in $D_\mathit{valid}$.
    \begin{equation*}
        RMSE(H, \altmathcal{D}_\mathit{valid}) = \sqrt{\frac{1}{|D_\mathit{valid}|}~\mathlarger{\sum}_{(x, y) \in D_\mathit{valid}} (H(x) - y)^2}
    \end{equation*}
\end{example}

\subsection{Unsupervised Learning}

For unsupervised tasks: k-means \cite{} and isolation forest \cite{}.

\begin{example}[K-Means]
    The k-means algorithm aims to partition the instances into a predetermined number of clusters based on their feature similarity.
    Being unsupervised, the algorithm iteratively assigns instances to clusters and updates the cluster centroids until its convergence criteria i.e., instances in the same cluster are more similar than instances in a different cluster.
    Hyperparameters such as the number of clusters and the similarity measure affect the returned partitioning.
\end{example}

\section{Automated Machine Learning}\label{automl-background-sec:automl}


% \begin{itemize}
%     \item Qui ci va sicuramente la formalizzazione
%     \item la motivation è dubbia, forse ripeteremmo quello che è scritto sopra nell'Introduction
% \end{itemize}



\part{Human-centered AutoML}\label{part:automl}


\chapter{Formalization}\label{automl-chap:formalization}

% \include{part-automl/chapter-formalization/main}
\include{part-automl/chapter-supervised/main}
% \include{part-automl/chapter-unsupervised/main}
% \include{part-automl/chapter-moo/main}
% \include{part-automl/chapter-llm/main}


\chapter{Unsupervised Learning}
\label{automl-chap:unsupervised}

\chapter{Multi-objective Optimization}
\label{automl-chap:moo}

\chapter{AutoML in the Age of Large Language Models}
\label{automl-chap:llm}
\begin{itemize}
    \item Questo qua o lo mettiamo a parte??
\end{itemize}

\part{Physics-coupled AutoML}\label{part:farming}

\chapter{Background and Formalization}
\label{precision-chap:background}
% \section{Motivation and contributions}
% \section{Structure}

\chapter{Data Platform Architecture}
\label{precision-chap:architecture}

\chapter{Orchard3dLab}
\label{precision-chap:orchard}

\chapter{PLUTO}
\label{precision-chap:pluto}

\chapter{Forecasting ???}
\label{precision-chap:forecasting}

\chapter{Smart-irrigation ???}
\label{precision-chap:smart-irrigation}

% \part{Unconventional Data}\label{part:trajectory}
% \include{part-trajectory/chapter-introduction/main}
% \include{part-trajectory/chapter-mapmatching/main}
% \include{part-trajectory/chapter-survey/main}
% \include{part-trajectory/chapter-dart/main}
% \include{part-trajectory/chapter-conclusion/main}

% \part{Advanced Analytics}\label{part:olap}
% \include{part-olap/chapter-introandback/main}
% \include{part-olap/chapter-augmented/main}
% \include{part-olap/chapter-conversational/main}
% \include{part-olap/chapter-describe/main}
% \include{part-olap/chapter-summarization/main}
% \include{part-olap/chapter-conclusion/main}

% Backmatter should be commented out, if you are using appendices after References
%\backmatter

% ********************************** Bibliography ******************************
\begin{spacing}{.8}
% To use the conventional natbib style referencing
% Bibliography style previews: http://nodonn.tipido.net/bibstyle.php
% Reference styles: http://sites.stat.psu.edu/~surajit/present/bib.htm
\bibliographystyle{apalike}
%\bibliographystyle{unsrt} % Use for unsorted references  
%\bibliographystyle{plainnat} % use this to have URLs listed in References
\cleardoublepage
% \nocite{*}
\bibliography{global/refs} % Path to your References.bib file

% If you would like to use BibLaTeX for your references, pass `custombib' as
% an option in the document class. The location of 'reference.bib' should be
% specified in the preamble.tex file in the custombib section.
% Comment out the lines related to natbib above and uncomment the following line.
%\printbibliography[heading=bibintoc, title={References}]
\end{spacing}

% ********************************** Appendices ********************************
% \begin{appendices} % Using appendices environment for more functunality
% \include{Appendix1/appendix1}
% \include{Appendix2/appendix2}
% \end{appendices}

% *************************************** Index ********************************
% \printthesisindex % If index is present
\end{document}
